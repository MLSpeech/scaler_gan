import logging
from typing import Optional, List, Tuple, Union

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as f
from torch.autograd import Variable

from scaler_gan.scalergan_utils.scalergan_utils import homography_based_on_top_corners_x_shift, homography_grid


def weights_init(module: nn.Module):
    """
     This is used to initialize weights of any network
    :param module: Module object that it's weight should be initialized
    :return:
    """
    class_name = module.__class__.__name__
    if class_name.find("Conv") != -1:
        nn.init.xavier_normal_(module.weight, 0.01)
        if hasattr(module.bias, "data"):
            module.bias.data.fill_(0)
    elif class_name.find("nn.BatchNorm2d") != -1:
        module.weight.data.normal_(1.0, 0.02)
        module.bias.data.fill_(0)

    elif class_name.find("LocalNorm") != -1:
        module.weight.data.normal_(1.0, 0.02)
        module.bias.data.fill_(0)


class LocalNorm(nn.Module):
    """
    Local Normalization class
    """

    def __init__(self, num_features: int):
        """
        Init
        :param num_features: Number of features
        """
        super(LocalNorm, self).__init__()
        self.weight = nn.Parameter(torch.Tensor(num_features))
        self.bias = nn.Parameter(torch.Tensor(num_features))
        self.get_local_mean = nn.AvgPool2d(33, 1, 16, count_include_pad=False)

        self.get_var = nn.AvgPool2d(33, 1, 16, count_include_pad=False)

    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """
        Feed-forward run
        :param input_tensor: The input tensor
        :return: The normalized tenosr
        """
        local_mean = self.get_local_mean(input_tensor)
        print(local_mean)
        centered_input_tensor = input_tensor - local_mean
        print(centered_input_tensor)
        squared_diff = centered_input_tensor ** 2
        print(squared_diff)
        local_std = self.get_var(squared_diff) ** 0.5
        print(local_std)
        normalized_tensor = centered_input_tensor / (local_std + 1e-8)

        return normalized_tensor  # * self.weight[None, :, None, None] + self.bias[None, :, None, None]


class GANLoss(nn.Module):
    """ Receiving the final layer form the discriminator and a boolean indicating whether the input to the
     discriminator is real or fake (generated by generator), this returns a patch"""

    def __init__(self):
        """
        Init
        """
        super(GANLoss, self).__init__()

        # Initialize label tensor
        self.label_tensor = None

        # Loss tensor is prepared in network initialization.
        # Note: When activated as a loss between to feature-maps, then a loss-map is created. However, using defaults
        # for BCEloss, this map is averaged and reduced to a single scalar
        self.loss = nn.MSELoss()

    def forward(self, d_last_layer: torch.Tensor, is_d_input_real: bool) -> torch.Tensor:
        """
        Feed-forward run
        :param d_last_layer: The final layer form the discriminator
        :param is_d_input_real: boolean indicating whether the input to the discriminator is real or fake
        :return: The GAN loss
        """
        # Determine label map according to whether current input to discriminator is real or fake
        self.label_tensor = (
                Variable(torch.ones_like(d_last_layer), requires_grad=False)
                * is_d_input_real
        )

        # Finally return the loss
        return self.loss(d_last_layer, self.label_tensor)


class WeightedMSELoss(nn.Module):
    """
    Weighted MSE Loss
    """

    def __init__(self, use_l1: Optional[bool] = False):
        """
        Init
        :param use_l1: Flag for use l1 loss or not
        """
        super(WeightedMSELoss, self).__init__()

        self.unweighted_loss = nn.L1Loss() if use_l1 else nn.MSELoss()

    def forward(self, input_tensor: torch.Tensor, target_tensor: torch.Tensor, loss_mask: torch.Tensor) -> torch.Tensor:
        """
        Feed forward run
        :param input_tensor: The input tensor
        :param target_tensor: The output tensor
        :param loss_mask: The loss mask tensor
        :return: The Weighted loss
        """
        if loss_mask is not None:
            e = (target_tensor.detach() - input_tensor) ** 2
            e *= loss_mask
            return torch.sum(e) / torch.sum(loss_mask)
        else:
            return self.unweighted_loss(input_tensor, target_tensor)


class MultiScaleLoss(nn.Module):
    """
    Multiscale Loss
    """

    def __init__(self):
        """
        Init
        """
        super(MultiScaleLoss, self).__init__()

        self.mse = nn.MSELoss()

    def forward(self, input_tensor: torch.Tensor, target_tensor: torch.Tensor,
                scale_weights: torch.Tensor) -> torch.Tensor:
        """
        Feed forward run
        :param input_tensor: The input tensor
        :param target_tensor: The target Tensor
        :param scale_weights: The scale weights tensor
        :return: The multiscale loss tensor
        """

        # Run all nets over all scales and aggregate the interpolated results
        loss = torch.tensor(0)
        for i, scale_weight in enumerate(scale_weights):
            input_tensor = f.interpolate(
                input_tensor, scale_factor=self.scale_factor ** (-i), mode="bilinear", align_corners=False
            )
            loss += scale_weight * self.mse(input_tensor, target_tensor)
        return loss


class Generator(nn.Module):
    """ Architecture of the Generator, uses res-blocks """

    def __init__(
            self,
            base_channels: Optional[int] = 64,
            n_blocks: Optional[int] = 6,
            n_downsampling: Optional[int] = 3,
            use_bias: Optional[bool] = True,
            skip_flag: Optional[bool] = True,
    ):
        """
        Init
        :param base_channels: The base number of channels
        :param n_blocks: The number of res blocks
        :param n_downsampling: The number of downsampling blocks
        :param use_bias: Use bias or not
        :param skip_flag: Use skip connections or not
        """
        super(Generator, self).__init__()

        # Determine whether to use skip connections
        self.skip = skip_flag

        # Entry block
        # First conv-block, no stride so image dims are kept and channels dim is expanded (pad-conv-norm-relu)
        self.entry_block = nn.Sequential(
            nn.ReflectionPad2d(3),
            nn.utils.spectral_norm(
                nn.Conv2d(1, base_channels, kernel_size=7, bias=use_bias)
            ),
            nn.BatchNorm2d(base_channels),
            nn.LeakyReLU(0.2, True),
        )

        # Geometric transformation
        self.geo_transform = GeoTransform()

        # Downscaling
        # A sequence of strided conv-blocks. Image dims shrink by 2, channels dim expands by 2 at each block
        self.downscale_block = RescaleBlock(n_downsampling, 0.5, base_channels, True)

        # Bottleneck
        # A sequence of res-blocks
        bottleneck_block = []
        for _ in range(n_blocks):
            # noinspection PyUnboundLocalVariable
            bottleneck_block += [
                ResnetBlock(base_channels * 2 ** n_downsampling, use_bias=use_bias)
            ]
        self.bottleneck_block = nn.Sequential(*bottleneck_block)

        # Upscaling
        # A sequence of transposed-conv-blocks, Image dims expand by 2, channels dim shrinks by 2 at each block\
        self.upscale_block = RescaleBlock(n_downsampling, 2.0, base_channels, True)

        # Final block
        # No stride so image dims are kept and channels dim shrinks to 3 (output image channels)
        self.final_block = nn.Sequential(
            # nn.ReflectionPad2d(3), nn.Conv2d(base_channels, 1, kernel_size=7), nn.Tanh()
            # TODO: without Tanh, for not having output [-1,1]
            nn.ReflectionPad2d(3), nn.Conv2d(base_channels, 1, kernel_size=7)
        )

    def forward(self, input_tensor: torch.Tensor, output_size: int, random_affine: List[float]) -> torch.Tensor:
        """
        Feed forward run
        :param input_tensor: The input Tensor
        :param output_size: The output size
        :param random_affine: List of random affine numbers
        :return: The output tensor
        """
        # A condition for having the output at same size as the scaled input is having even output_size

        # Entry block
        feature_map = self.entry_block(input_tensor)

        # Change scale to output scale by interpolation
        if random_affine is None:
            feature_map = f.interpolate(feature_map, size=output_size, mode="bilinear", align_corners=False)
        else:
            feature_map = self.geo_transform(
                feature_map, output_size, random_affine
            )

        # Downscale block
        feature_map, downscales = self.downscale_block(
            feature_map, return_all_scales=self.skip
        )

        # Bottleneck (res-blocks)
        feature_map = self.bottleneck_block(feature_map)

        # Upscale block
        feature_map, _ = self.upscale_block(
            feature_map, pyramid=downscales, skip=self.skip
        )

        # Final block
        output_tensor = self.final_block(feature_map)

        return output_tensor


class ResnetBlock(nn.Module):
    """ A single Res-Block module """

    def __init__(self, dim: int, use_bias: bool):
        """
        Init
        :param dim: The dimension
        :param use_bias: Flag to use bias or not
        """
        super(ResnetBlock, self).__init__()

        # A res-block without the skip-connection, pad-conv-norm-relu-pad-conv-norm
        self.conv_block = nn.Sequential(
            nn.utils.spectral_norm(
                nn.Conv2d(dim, dim // 4, kernel_size=1, bias=use_bias)
            ),
            nn.BatchNorm2d(dim // 4),
            nn.LeakyReLU(0.2, True),
            nn.ReflectionPad2d(1),
            nn.utils.spectral_norm(
                nn.Conv2d(dim // 4, dim // 4, kernel_size=3, bias=use_bias)
            ),
            nn.BatchNorm2d(dim // 4),
            nn.LeakyReLU(0.2, True),
            nn.utils.spectral_norm(
                nn.Conv2d(dim // 4, dim, kernel_size=1, bias=use_bias)
            ),
            nn.BatchNorm2d(dim),
        )

    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """
        Feed forward run
        :param input_tensor: The input tensor
        :return: The output tensor
        """
        # The skip connection is applied here
        return input_tensor + self.conv_block(input_tensor)


class MultiScaleDiscriminator(nn.Module):
    """
    The Multiscale Discriminator class
    """

    def __init__(
            self,
            real_crop_size: int,
            max_n_scales: Optional[int] = 9,
            scale_factor: Optional[int] = 2,
            base_channels: Optional[int] = 128,
            extra_conv_layers: Optional[int] = 0,
    ):
        """
        Init
        :param real_crop_size: real crop size
        :param max_n_scales: Maximum number of scales
        :param scale_factor: scale factor
        :param base_channels: The base number of channels
        :param extra_conv_layers: Number of extra conv layers
        """
        super(MultiScaleDiscriminator, self).__init__()
        self.base_channels = base_channels
        self.scale_factor = scale_factor
        self.min_size = 16
        self.extra_conv_layers = extra_conv_layers

        # We want the max num of scales to fit the size of the real examples. further scaling would create networks that
        # only train on fake examples
        self.max_n_scales = np.min(
            [
                np.int(
                    np.ceil(
                        np.log(np.min(real_crop_size) * 1.0 / self.min_size)
                        / np.log(self.scale_factor)
                    )
                ),
                max_n_scales,
            ]
        )

        # Prepare a list of all the networks for all the wanted scales
        self.nets = nn.ModuleList()

        # Create a network for each scale
        for _ in range(self.max_n_scales):
            self.nets.append(self.make_net())

    def make_net(self):
        """
        Make net
        :return: Sequential module
        """
        base_channels = self.base_channels
        net = []

        # Entry block
        net += [
            nn.utils.spectral_norm(
                nn.Conv2d(1, base_channels, kernel_size=3, stride=1)
            ),
            nn.BatchNorm2d(base_channels),
            nn.LeakyReLU(0.2, True),
        ]

        # Downscaling blocks
        # A sequence of strided conv-blocks. Image dims shrink by 2, channels dim expands by 2 at each block
        net += [
            nn.utils.spectral_norm(
                nn.Conv2d(base_channels, base_channels * 2, kernel_size=3, stride=2)
            ),
            nn.BatchNorm2d(base_channels * 2),
            nn.LeakyReLU(0.2, True),
        ]

        # Regular conv-block
        net += [
            nn.utils.spectral_norm(
                nn.Conv2d(
                    in_channels=base_channels * 2,
                    out_channels=base_channels * 2,
                    kernel_size=3,
                    bias=True,
                )
            ),
            nn.BatchNorm2d(base_channels * 2),
            nn.LeakyReLU(0.2, True),
        ]

        # Additional 1x1 conv-blocks
        for _ in range(self.extra_conv_layers):
            net += [
                nn.utils.spectral_norm(
                    nn.Conv2d(
                        in_channels=base_channels * 2,
                        out_channels=base_channels * 2,
                        kernel_size=3,
                        bias=True,
                    )
                ),
                nn.BatchNorm2d(base_channels * 2),
                nn.LeakyReLU(0.2, True),
            ]

        # Final conv-block
        # Ends with a Sigmoid to get a range of 0-1
        net += nn.Sequential(
            nn.utils.spectral_norm(nn.Conv2d(base_channels * 2, 1, kernel_size=1)),
            nn.Sigmoid(),
        )

        # Make it a valid layers sequence and return
        return nn.Sequential(*net)

    def forward(self, input_tensor: torch.Tensor, scale_weights: torch.Tensor) -> torch.Tensor:
        """
        Feed forward run
        :param input_tensor: The input tensor
        :param scale_weights: The scale weights tensor
        :return: The output tensor
        """
        aggregated_result_maps_from_all_scales = (
                self.nets[0](input_tensor) * scale_weights[0]
        )
        map_size = aggregated_result_maps_from_all_scales.shape[2:]

        logger = logging.getLogger()
        # Run all nets over all scales and aggregate the interpolated results
        for net, scale_weight, i in zip(
                self.nets[1:], scale_weights[1:], range(1, len(scale_weights))
        ):
            downscaled_image = f.interpolate(
                input_tensor, scale_factor=self.scale_factor ** (-i), mode="bilinear", align_corners=False
            )
            result_map_for_current_scale = None
            try:
                result_map_for_current_scale = net(downscaled_image)
            except KeyboardInterrupt:
                raise
            except Exception as e:
                print(f"Something went wrong in epoch {i}, While training.")
                print(
                    f"epoch in net: {i}, downscaled_image shape: {downscaled_image.shape}"
                )
                raise
            try:
                upscaled_result_map_for_current_scale = f.interpolate(
                    result_map_for_current_scale, size=map_size, mode="bilinear", align_corners=False
                )
            except:
                logger.error(
                    f"-------- ERROR --------\n input tensor shape: {input_tensor.shape}"
                )
                logger.error(
                    f"downscaled_image shape after first interpulate : {downscaled_image.shape}\n epoch: {i}"
                )
                logger.error(
                    f"result map shape after NET: {result_map_for_current_scale.shape}"
                )
                raise
            aggregated_result_maps_from_all_scales += (
                    upscaled_result_map_for_current_scale * scale_weight
            )

        return aggregated_result_maps_from_all_scales


class RescaleBlock(nn.Module):
    """
    Rescale Block class
    """

    def __init__(self, n_layers: int, scale: Optional[float] = 0.5, base_channels: Optional[int] = 64,
                 use_bias: Optional[bool] = True):
        """
        Init
        :param n_layers: The number of layers
        :param scale: Scale factor
        :param base_channels: Base number of channels
        :param use_bias: Flag to use bias or not
        """
        super(RescaleBlock, self).__init__()

        self.scale = scale

        self.conv_layers = [None] * n_layers

        in_channel_power = scale > 1
        out_channel_power = scale < 1
        i_range = range(n_layers) if scale < 1 else range(n_layers - 1, -1, -1)

        for i in i_range:
            self.conv_layers[i] = nn.Sequential(
                nn.ReflectionPad2d(1),
                nn.utils.spectral_norm(
                    nn.Conv2d(
                        in_channels=base_channels * 2 ** (i + in_channel_power),
                        out_channels=base_channels * 2 ** (i + out_channel_power),
                        kernel_size=3,
                        stride=1,
                        bias=use_bias,
                    )
                ),
                nn.BatchNorm2d(base_channels * 2 ** (i + out_channel_power)),
                nn.LeakyReLU(0.2, True))

            self.add_module("conv_%d" % i, self.conv_layers[i])

        if scale > 1:
            self.conv_layers = self.conv_layers[::-1]

        self.max_pool = nn.MaxPool2d(2, 2)

    def forward(self, input_tensor: torch.Tensor,
                pyramid: Optional[torch.Tensor] = None,
                return_all_scales: Optional[bool] = False,
                skip: Optional[bool] = False) -> Tuple[torch.Tensor, Optional[List[torch.Tensor]]]:
        """

        :param input_tensor: The input tensor
        :param pyramid: The pyramid tensor
        :param return_all_scales: Flag to return all scales
        :param skip: Flag to skip or not
        :return: Tuple with feature maps and all scales (if return_all_scales is True)
        """
        feature_map = input_tensor
        all_scales = []
        if return_all_scales:
            all_scales.append(feature_map)

        for i, conv_layer in enumerate(self.conv_layers):

            if self.scale > 1.0:
                feature_map = f.interpolate(
                    feature_map, scale_factor=self.scale, mode="nearest"
                )

            feature_map = conv_layer(feature_map)

            if skip:
                feature_map = feature_map + pyramid[-i - 2]

            if self.scale < 1.0:
                feature_map = self.max_pool(feature_map)

            if return_all_scales:
                all_scales.append(feature_map)

        return (feature_map, all_scales) if return_all_scales else (feature_map, None)


class RandomCrop(nn.Module):
    """
    Random Crop class
    """
    def __init__(self, crop_size: Optional[List[int]] = None, return_pos: Optional[bool] = False,
                 must_divide: Optional[float] = 4.0):
        """
        Init
        :param crop_size: The crop size
        :param return_pos: Flag to return position
        :param must_divide: division factor
        """
        super(RandomCrop, self).__init__()

        # Determine crop size
        self.crop_size = crop_size
        self.must_divide = must_divide
        self.return_pos = return_pos

    def forward(self, input_tensor: torch.Tensor,
                crop_size: Optional[List[int]] = None) -> Union[torch.Tensor, Tuple[torch.Tensor, Tuple[int, int]]]:
        """
        Feed forward run
        :param input_tensor: The input tensor
        :param crop_size: Optional crop size
        :return: The output tensor if self.return_pos is False otherwise return the output tensor and positions
        """
        # im_v_sz, im_h_sz = input_tensors[0].shape[2:]
        im_v_sz, im_h_sz = input_tensor.shape[2:]
        if crop_size is None:
            cr_v_sz, cr_h_sz = np.clip(
                self.crop_size, [0, 0], [im_v_sz - 1, im_h_sz - 1]
            )
            cr_v_sz, cr_h_sz = np.uint32(
                np.floor(np.array([cr_v_sz, cr_h_sz]) * 1.0 / self.must_divide)
                * self.must_divide
            )
        else:
            cr_v_sz, cr_h_sz = crop_size

        top_left_v, top_left_h = [
            np.random.randint(0, im_v_sz - cr_v_sz),
            np.random.randint(0, im_h_sz - cr_h_sz),
        ]

        out_tensor = input_tensor[
                     :, :, top_left_v: top_left_v + cr_v_sz, top_left_h: top_left_h + cr_h_sz,
                     ]

        # out_tensors = [input_tensor[:, top_left_v:top_left_v + cr_v_sz, top_left_h:top_left_h + cr_h_sz]
        #                if input_tensor is not None else None for input_tensor in input_tensors]

        return (out_tensor, (top_left_v, top_left_h)) if self.return_pos else out_tensor


class GeoTransform(nn.Module):
    """
    Geo Transform class
    """
    def __init__(self):
        """
        Init
        """
        super(GeoTransform, self).__init__()

    def forward(self, input_tensor: torch.Tensor, target_size: List[int], shifts: List[float]) -> torch.Tensor:
        """
        Feed forward run
        :param input_tensor: The input tensor
        :param target_size: The target size (list of integers)
        :param shifts: shifts (list of floats)
        :return: The output tensor
        """
        sz = input_tensor.shape
        theta = homography_based_on_top_corners_x_shift(shifts).to(input_tensor.device)

        pad = f.pad(
            input_tensor,
            (
                np.abs(np.int(np.ceil(sz[3] * shifts[0]))),
                np.abs(np.int(np.ceil(-sz[3] * shifts[1]))),
                0,
                0,
            ),
            "reflect",
        )
        target_size4d = torch.Size(
            [pad.shape[0], pad.shape[1], target_size[0], target_size[1]]
        )

        grid = homography_grid(theta.expand(pad.shape[0], -1, -1), target_size4d)

        return f.grid_sample(pad, grid, mode="bilinear", padding_mode="border", align_corners=True)
